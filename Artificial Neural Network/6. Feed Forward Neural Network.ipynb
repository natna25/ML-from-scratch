{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimages = []\\ndata_dir = \"data/\"\\nfile1_name = \"image_0.txt\"\\n\\n\\nimages = [[float(pix) for pix in img.split(\",\")] for img in open(data_dir+file1_name)]\\n\\nimages = np.reshape(images,(5000,20,20)) #reshaping the arry\\nimages = [np.rot90(img) for img in images] #rotating it by 90 deg\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "images = []\n",
    "data_dir = \"data/\"\n",
    "file1_name = \"image_0.txt\"\n",
    "\n",
    "\n",
    "images = [[float(pix) for pix in img.split(\",\")] for img in open(data_dir+file1_name)]\n",
    "\n",
    "images = np.reshape(images,(5000,20,20)) #reshaping the arry\n",
    "images = [np.rot90(img) for img in images] #rotating it by 90 deg\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.imshow(images[3000], cmap=\"bone\",origin =\"lower\")\\nplt.show()\\nplt.imshow(images[4000], cmap=\"bone\",origin =\"lower\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.imshow(images[3000], cmap=\"bone\",origin =\"lower\")\n",
    "plt.show()\n",
    "plt.imshow(images[4000], cmap=\"bone\",origin =\"lower\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_label(y):\n",
    "    if y == 10:\n",
    "        y=0\n",
    "    label = np.zeros(10)\n",
    "    #print(type(y))\n",
    "    label[y] = 1\n",
    "    #print(label)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "data_dir = \"data/\"\n",
    "data_file = \"image_0.txt\"\n",
    "label_file = \"label.txt\"\n",
    "\n",
    "X = [[float(pix) for pix in  img.split(\",\") ] for img in open(data_dir+data_file)]\n",
    "y = [vectorize_label(int(label)) for label in open(data_dir+label_file)]\n",
    "\n",
    "X = [np.reshape(x,(400,1)) for x in X]\n",
    "\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## displaying an image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18549300240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = X[1000]\n",
    "img = np.reshape(img,(20,20)) #reshaping the vector to be 20x20\n",
    "img = np.rot90(img) #rotate by 90° the array to be readable \n",
    "\n",
    "plt.imshow(img, cmap=\"bone\",origin =\"lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499 3499\n",
      "1501 1501\n"
     ]
    }
   ],
   "source": [
    "train_indexes = list()\n",
    "test_indexes = list()\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if np.random.uniform(0,1) < threshold:\n",
    "        train_indexes += [i]\n",
    "    else:\n",
    "        test_indexes += [i]\n",
    "\n",
    "X_train = [np.array(X[i]) for i in train_indexes ]\n",
    "y_train = [y[i] for i in train_indexes ]\n",
    "\n",
    "X_test = [np.array(X[i]) for i in test_indexes ]\n",
    "y_test = [y[i] for i in test_indexes ]\n",
    "\n",
    "\n",
    "print(len(X_train),len(y_train))\n",
    "print(len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network\n",
    "\n",
    "For the sake of simplicity, our network will only have 3 layers, 1 for the input, 1 that is 'hidden' and a last one for the outputs. The respective dimensions (number of neurons/inputs) of each of these layes will be:\n",
    "$$ [400,30,10]$$\n",
    "\n",
    "Each layer, except the first one has a weight matrix (because each neuron of each layer is connected to every neuron of the previous layer) and a bias vector (bias being the intercept in our linear function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dimensions = [400,30,10]\n",
    "num_layers = len(network_dimensions)\n",
    "#initialise biases according to a normal distribution of mean 0 and variance 1\n",
    "biases = [np.random.randn(x,1) for x in network_dimensions[1:] ] #creates a matrixs of biases for each layer except the first\n",
    "#same for the weights except the matrix is larger\n",
    "#creates a l-1 X l matrix for each layer except the first one, each neuron in layer l will have as many weights as there are neurons in l-1 \n",
    "weights = [np.random.rand(y,x) for x,y in zip(network_dimensions[:-1],network_dimensions[1:])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 400 Y: 30\n",
      "X: 30 Y: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x,y in zip(network_dimensions[:-1],network_dimensions[1:]):\n",
    "    print(\"X:\",x,'Y:',y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "there are 2 big steps for the neural network, first we perform forward propagation, in this step we feed the inputs though the 1st layer, these inputs then are passed through the network in order to activate the second layer and so on.\n",
    "The second step is backpropagation where we modify the model's parameters after having calculated the SSE and performing gradient descent.\n",
    "\n",
    "the main steps to forward propagation are:\n",
    "\n",
    "1) take input\n",
    "\n",
    "2) multiply by a weight\n",
    "\n",
    "3) add a bias\n",
    "\n",
    "4) pass through activation function\n",
    "\n",
    "5) feed output of activation to the next layer and so on until reaching the output\n",
    "\n",
    "### activation function\n",
    "There are a few choices out there of what kind of activation function we can use on our neural network like Relu, Tanh or sigmoid. We are using the sigmoid or logistic function\n",
    "$$ h(z)= \\frac{1}{1+e^{-z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed forward:\n",
    "this step consists in taking in an input and propagating it through the network. We return the vector of outputs that will classify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(a,weights,biases):\n",
    "    a = np.reshape(a,(400,1))\n",
    "    for b,w in zip(biases,weights):\n",
    "        \n",
    "        #print(w.shape,\"*\",a.shape)\n",
    "        #print(b.shape)\n",
    "        a = sigmoid(np.dot(w,a)+b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999958]\n",
      " [0.99999835]\n",
      " [0.99999999]\n",
      " [0.99999984]\n",
      " [0.99999958]\n",
      " [0.99999945]\n",
      " [0.99999504]\n",
      " [0.99999978]\n",
      " [0.99999995]\n",
      " [0.99999694]]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "im = X_train[0]\n",
    "out = y_train[0]\n",
    "\n",
    "pred = feed_forward(im,weights,biases)\n",
    "\n",
    "print(pred)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the network\n",
    "To train our network we need to progressively improve the accurancy of the network and minimize the Cost function. \n",
    "\n",
    "The method in which we are modifying the weights and biases in order to minimize the cost function is by using backpropagation. backproagation consists in propagaing the error bakwards and calculating its partial derivative with respect to the weights and biases. With this method, we can change the value of the weights one by one until we converge towards a local (or global minimum)\n",
    "\n",
    "We first need to define some terms and a specific notation.\n",
    "$$ w^l_{j,k} $$ \n",
    "Where:\n",
    "- l layer number\n",
    "- k index of neuron in l-1 layer\n",
    "- j index of neuron in lth layer\n",
    "therefore, this denotes the weight in the lth layer going from the K th neuron in layer l-1 to J th neuron in layer l\n",
    "\n",
    "Then, we define the activation of the J th neuron in layer l as:\n",
    "$$ a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right) $$\n",
    "\n",
    "\n",
    "Next, we will define a total of 4 equations that will help us calculate the partial derivatives necessary to perform backropagation. \n",
    "\n",
    "1) $$  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j) $$\n",
    "\n",
    "where delta represents the error derivative in the output layer L\n",
    "\n",
    "We can also show this function in a simpler notation with: $ \\delta^L = \\nabla_a C \\odot \\sigma'(z^L) $ where $\\odot$ is the hadamard product (element-wise product)\n",
    "\n",
    "2) $$\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l)$$\n",
    "\n",
    "Where the error derivative from any layer relates to the layers further down the network. The terms $w^{l+1})^T \\delta^{l+1}$ can be intuitively understood as propagating backwards the prediction error\n",
    "\n",
    "3) $$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j $$\n",
    "\n",
    "The rate of change of the cost with respects to the biases in the network\n",
    "\n",
    "\n",
    "4)  $$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$$\n",
    "\n",
    "The rate of change of the cost with respects to the weights in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    #computes the derivative of the sigmoid function\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_prime(y_pred,y):\n",
    "    #computes the derivative of the cost function which is simply the difference between our predction and the actual y value\n",
    "    y = np.reshape(y,(10,1))\n",
    "    return (y_pred-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSE(y_pred,y):\n",
    "    error = 0\n",
    "    for pred,real in zip(y_pred,y):\n",
    "        error += (pred-real)**2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(x,y,weights,biases): \n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    \n",
    "    a = np.reshape(x,(400,1)) #reshaping the 'image' so its dimensions are proper for matrix multiplication\n",
    "    activations = [] # we store the activation at each layer\n",
    "    activations = [a] #first activation is our inputs\n",
    "    Zs = [] # we store the weighted sum as well \n",
    "    \n",
    "    #feed forward part\n",
    "    for b,w in zip(biases,weights):\n",
    "        z = np.dot(w,a) + b # calculate weighted sum\n",
    "        Zs.append(z) #append it for later\n",
    "        a = sigmoid(z) # calculate activation\n",
    "       \n",
    "        activations.append(a) # save activation for computing gradient \n",
    "    \n",
    "    ## backward pass \n",
    "    \n",
    "    delta_L = cost_prime(activations[-1],y) * sigmoid_prime(Zs[-1]) #see equation 1 above\n",
    "    \n",
    "    #inserting this delta to compute equations 3 and 4 (respectively)\n",
    "    nabla_b[-1] = delta_L\n",
    "    nabla_w[-1] = np.dot(delta_L,activations[-2].transpose())\n",
    "    \n",
    "    # propagate the error bakwards to compute the weight and bias gradient for all layers\n",
    "    # we use the negative indexing of python to reffer to layers from the end to the front\n",
    "    delta_l = delta_L\n",
    "    for l in range(2,num_layers):\n",
    "        Z_l = Zs[-l]\n",
    "        delta_l = np.dot(weights[-l+1].transpose(), delta_l) * sigmoid_prime(Z_l) # see equation 3\n",
    "        \n",
    "        nabla_b[-l] = delta_l\n",
    "        nabla_w[-l] = np.dot(delta_l,activations[-l - 1].transpose())\n",
    "        \n",
    "        #print(nabla_w[1])\n",
    "    \n",
    "    return nabla_w,nabla_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stochastic gradient descent\n",
    "\n",
    "To train our NN, we will be implementing stochastic gradient descent. Instead of taking the average SSE (MSE) and using this value for backpropagation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X,y,weights,biases,learning_rate,nb_epochs,batch_size,test_data):\n",
    "    epoch_cost = []\n",
    "    costs = [] #to keep track of our sse over the epochs\n",
    "    training_data = list(zip(X,y))\n",
    "    train_samples = len(X)\n",
    "        \n",
    "    for epoch in range(nb_epochs):\n",
    "        print(\"starting epoch:\",epoch,\"score:\",eval(test_data,weights,biases),\"/\",len(test_data))\n",
    "        #making up our batches\n",
    "        \n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k:k+batch_size]  for k in range(0, train_samples, batch_size)]\n",
    "        #print(mini_batches)\n",
    "        #iterate over our batches\n",
    "        for batch in mini_batches:\n",
    "            X_batch,y_batch = zip(*batch) #unzip our batch back to input and labels\n",
    "            weights,biases = update_batch(X_batch,y_batch,weights,biases)\n",
    "            \n",
    "    \n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_batch(X,y,weights,biases):\n",
    "    #feedforward part, we also keep the activation of each layer of neurons \n",
    "    #so that it is simpler when performing backpropagation later\n",
    "    nabla_weights = [np.zeros(w.shape) for w in weights]\n",
    "    nabla_biases   = [np.zeros(b.shape) for b in biases]\n",
    "\n",
    "    for i,x in enumerate(X):\n",
    "        #gets partial derivatives for weights and biases with respects to the cost\n",
    "        delta_nabla_weights,delta_nabla_biases = backpropagate(x,y[i],weights,biases)\n",
    "\n",
    "        # add the delta calculated above to the nabla w and b, we will average the changes later\n",
    "        nabla_biases = [nb+dnb for nb,dnb in zip(nabla_biases,delta_nabla_biases)]\n",
    "        nabla_weights = [nw+dnw for nw,dnw in zip(nabla_weights,delta_nabla_weights)]\n",
    "\n",
    "    #after having completed all the train examples, we update the weights and biases before the next epoch\n",
    "    # we average the changes given by the consecutive backpropagations and apply this change to the w and b taking into account the learning rate\n",
    "    biases = [b - (learning_rate/len(X))*nb for b,nb in zip(biases,nabla_biases)]\n",
    "    weights = [w - (learning_rate/len(X))*nw for w,nw in zip(weights,nabla_weights)]\n",
    "    \n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(train_data,weights,biases):\n",
    "    preds = [(np.argmax(feed_forward(x,weights,biases)),np.argmax(y)) for x,y in train_data ]\n",
    "    score = sum(int(x==y) for (x,y) in preds)\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 0 score: 172 / 1501\n",
      "starting epoch: 1 score: 172 / 1501\n",
      "starting epoch: 2 score: 172 / 1501\n",
      "starting epoch: 3 score: 172 / 1501\n",
      "starting epoch: 4 score: 172 / 1501\n",
      "starting epoch: 5 score: 172 / 1501\n",
      "starting epoch: 6 score: 172 / 1501\n",
      "starting epoch: 7 score: 172 / 1501\n",
      "starting epoch: 8 score: 172 / 1501\n",
      "starting epoch: 9 score: 172 / 1501\n",
      "starting epoch: 10 score: 172 / 1501\n",
      "starting epoch: 11 score: 172 / 1501\n",
      "starting epoch: 12 score: 172 / 1501\n",
      "starting epoch: 13 score: 172 / 1501\n",
      "starting epoch: 14 score: 172 / 1501\n",
      "starting epoch: 15 score: 172 / 1501\n",
      "starting epoch: 16 score: 172 / 1501\n",
      "starting epoch: 17 score: 172 / 1501\n",
      "starting epoch: 18 score: 172 / 1501\n",
      "starting epoch: 19 score: 172 / 1501\n",
      "starting epoch: 20 score: 172 / 1501\n",
      "starting epoch: 21 score: 172 / 1501\n",
      "starting epoch: 22 score: 172 / 1501\n",
      "starting epoch: 23 score: 172 / 1501\n",
      "starting epoch: 24 score: 172 / 1501\n",
      "starting epoch: 25 score: 172 / 1501\n",
      "starting epoch: 26 score: 172 / 1501\n",
      "starting epoch: 27 score: 172 / 1501\n",
      "starting epoch: 28 score: 172 / 1501\n",
      "starting epoch: 29 score: 172 / 1501\n",
      "starting epoch: 30 score: 172 / 1501\n",
      "starting epoch: 31 score: 172 / 1501\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9906b839a1d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-fd0d4faceba5>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(X, y, weights, biases, learning_rate, nb_epochs, batch_size, test_data)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#unzip our batch back to input and labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-d8227d757423>\u001b[0m in \u001b[0;36mupdate_batch\u001b[1;34m(X, y, weights, biases)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#gets partial derivatives for weights and biases with respects to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdelta_nabla_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta_nabla_biases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# add the delta calculated above to the nabla w and b, we will average the changes later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-842c13a804eb>\u001b[0m in \u001b[0;36mbackpropagate\u001b[1;34m(x, y, weights, biases)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mnabla_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta_l\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mnabla_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_l\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#print(nabla_w[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 10.0\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "test_data = list(zip(X_test,y_test))\n",
    "\n",
    "weights,biases = fit(X_train,y_train,weights,biases,learning_rate,n_epochs,batch_size,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test,y_test,weights,biases):\n",
    "    correct = 0\n",
    "    incorrect = 1\n",
    "    \n",
    "    for x,y in zip(X_test,y_test):\n",
    "        pred = feed_forward(x,weights,biases)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = X_train[1000]\n",
    "out = y_train[1000]\n",
    "\n",
    "pred = feed_forward(im,weights,biases)\n",
    "\n",
    "print(pred)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
