{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        accuracy = []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                accuracy.append((self.evaluate(test_data)/n_test)*100)\n",
    "                print (\"Epoch\",j,\": \",accuracy[j],\"%\")\n",
    "            else:\n",
    "                print (\"Epoch \",j,\" complete\")\n",
    "        \n",
    "        plt.plot(range(epochs),accuracy)\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #forward pass\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) \n",
    "                        for (x, y) in test_data]\n",
    "        tot = sum(int(x == y) for (x, y) in test_results)\n",
    "        return tot\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def feedback(self,number):\n",
    "        num_gen = np.zeros(10)\n",
    "        num_gen[number] = 1\n",
    "        num_gen = np.reshape(num_gen,(10,1))\n",
    "        a = num_gen\n",
    "        for w,b in zip(self.weights[::-1] ,self.biases[::-1]):\n",
    "            a = sigmoid(np.dot(w.transpose(), a)+b)\n",
    "        print(a.shape)\n",
    "        return a\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_label(y):\n",
    "    if y == 10:\n",
    "        y=0\n",
    "    label = np.zeros(10)\n",
    "    #print(type(y))\n",
    "    label[y] = 1\n",
    "    #print(label)\n",
    "    \n",
    "    return np.reshape(label,(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = []\n",
    "y = []\n",
    "data_dir = \"data/\"\n",
    "data_file = \"image_0.txt\"\n",
    "label_file = \"label.txt\"\n",
    "\n",
    "X = [[float(pix) for pix in  img.split(\",\") ] for img in open(data_dir+data_file)]\n",
    "y = [vectorize_label(int(label)) for label in open(data_dir+label_file)]\n",
    "\n",
    "X = [np.reshape(x,(400,1)) for x in X]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3527 3527\n",
      "1473 1473\n"
     ]
    }
   ],
   "source": [
    "train_indexes = list()\n",
    "test_indexes = list()\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if np.random.uniform(0,1) < threshold:\n",
    "        train_indexes += [i]\n",
    "    else:\n",
    "        test_indexes += [i]\n",
    "\n",
    "X_train = [np.array(X[i]) for i in train_indexes ]\n",
    "y_train = [y[i] for i in train_indexes ]\n",
    "\n",
    "X_test = [np.array(X[i]) for i in test_indexes ]\n",
    "y_test = [y[i] for i in test_indexes ]\n",
    "\n",
    "\n",
    "print(len(X_train),len(y_train))\n",
    "print(len(X_test),len(y_test))\n",
    "\n",
    "train_data = list(zip(X_train,y_train))\n",
    "test_data  = list(zip(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network([400,30,30,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :  14.663951120162933 %\n",
      "Epoch 1 :  18.87304820095044 %\n",
      "Epoch 2 :  28.445349626612355 %\n",
      "Epoch 3 :  35.1663272233537 %\n",
      "Epoch 4 :  40.25797691785472 %\n",
      "Epoch 5 :  44.12763068567549 %\n",
      "Epoch 6 :  46.50373387644263 %\n",
      "Epoch 7 :  49.15139171758317 %\n",
      "Epoch 8 :  51.32382892057027 %\n",
      "Epoch 9 :  53.69993211133741 %\n",
      "Epoch 10 :  55.397148676171085 %\n",
      "Epoch 11 :  57.97691785471826 %\n",
      "Epoch 12 :  59.877800407331975 %\n",
      "Epoch 13 :  61.98234894772573 %\n",
      "Epoch 14 :  62.661235573659205 %\n",
      "Epoch 15 :  65.30889341479973 %\n",
      "Epoch 16 :  66.32722335369992 %\n",
      "Epoch 17 :  68.09232858112695 %\n",
      "Epoch 18 :  69.04276985743381 %\n",
      "Epoch 19 :  69.65376782077392 %\n",
      "Epoch 20 :  71.21520706042091 %\n",
      "Epoch 21 :  71.62253903598099 %\n",
      "Epoch 22 :  72.50509164969449 %\n",
      "Epoch 23 :  73.45553292600135 %\n",
      "Epoch 24 :  74.47386286490156 %\n",
      "Epoch 25 :  74.60964019008826 %\n",
      "Epoch 26 :  75.49219280380176 %\n",
      "Epoch 27 :  75.6958587915818 %\n",
      "Epoch 28 :  76.78207739307535 %\n",
      "Epoch 29 :  76.91785471826205 %\n",
      "Epoch 30 :  77.05363204344874 %\n",
      "Epoch 31 :  77.59674134419552 %\n",
      "Epoch 32 :  77.73251866938222 %\n",
      "Epoch 33 :  78.41140529531569 %\n",
      "Epoch 34 :  78.61507128309573 %\n",
      "Epoch 35 :  79.0224032586558 %\n",
      "Epoch 36 :  79.76917854718262 %\n",
      "Epoch 37 :  79.76917854718262 %\n",
      "Epoch 38 :  79.70128988458927 %\n",
      "Epoch 39 :  80.44806517311609 %\n",
      "Epoch 40 :  80.78750848608283 %\n",
      "Epoch 41 :  80.92328581126952 %\n",
      "Epoch 42 :  81.80583842498302 %\n",
      "Epoch 43 :  81.73794976238969 %\n",
      "Epoch 44 :  81.12695179904956 %\n",
      "Epoch 45 :  81.73794976238969 %\n",
      "Epoch 46 :  82.68839103869654 %\n",
      "Epoch 47 :  81.87372708757637 %\n",
      "Epoch 48 :  82.4847250509165 %\n",
      "Epoch 49 :  83.02783435166327 %\n",
      "Epoch 50 :  83.36727766463 %\n",
      "Epoch 51 :  83.23150033944331 %\n",
      "Epoch 52 :  83.16361167684997 %\n",
      "Epoch 53 :  84.04616429056347 %\n",
      "Epoch 54 :  83.97827562797013 %\n",
      "Epoch 55 :  84.58927359131026 %\n",
      "Epoch 56 :  84.11405295315683 %\n",
      "Epoch 57 :  83.97827562797013 %\n",
      "Epoch 58 :  84.11405295315683 %\n",
      "Epoch 59 :  84.65716225390359 %\n",
      "Epoch 60 :  84.45349626612357 %\n",
      "Epoch 61 :  84.58927359131026 %\n",
      "Epoch 62 :  84.24983027834352 %\n",
      "Epoch 63 :  85.13238289205702 %\n",
      "Epoch 64 :  85.33604887983707 %\n",
      "Epoch 65 :  85.06449422946368 %\n",
      "Epoch 66 :  85.47182620502376 %\n",
      "Epoch 67 :  85.47182620502376 %\n",
      "Epoch 68 :  85.47182620502376 %\n",
      "Epoch 69 :  85.40393754243041 %\n",
      "Epoch 70 :  85.47182620502376 %\n",
      "Epoch 71 :  85.26816021724372 %\n",
      "Epoch 72 :  85.47182620502376 %\n",
      "Epoch 73 :  85.74338085539715 %\n",
      "Epoch 74 :  85.87915818058384 %\n",
      "Epoch 75 :  85.5397148676171 %\n",
      "Epoch 76 :  86.01493550577054 %\n",
      "Epoch 77 :  85.87915818058384 %\n",
      "Epoch 78 :  86.01493550577054 %\n",
      "Epoch 79 :  86.01493550577054 %\n",
      "Epoch 80 :  85.87915818058384 %\n",
      "Epoch 81 :  86.15071283095723 %\n",
      "Epoch 82 :  86.01493550577054 %\n",
      "Epoch 83 :  86.28649015614393 %\n",
      "Epoch 84 :  86.08282416836389 %\n",
      "Epoch 85 :  86.15071283095723 %\n",
      "Epoch 86 :  86.08282416836389 %\n",
      "Epoch 87 :  86.62593346911066 %\n",
      "Epoch 88 :  86.693822131704 %\n",
      "Epoch 89 :  86.62593346911066 %\n",
      "Epoch 90 :  86.8295994568907 %\n",
      "Epoch 91 :  86.42226748133062 %\n",
      "Epoch 92 :  86.76171079429736 %\n",
      "Epoch 93 :  86.62593346911066 %\n",
      "Epoch 94 :  86.62593346911066 %\n",
      "Epoch 95 :  86.76171079429736 %\n",
      "Epoch 96 :  86.693822131704 %\n",
      "Epoch 97 :  86.76171079429736 %\n",
      "Epoch 98 :  87.23693143245079 %\n",
      "Epoch 99 :  86.89748811948405 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 1.0\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "nn.SGD(train_data,n_epochs,batch_size,learning_rate,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct guesses: 1254\n",
      "incorrect guesses: 219\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct =0\n",
    "incorrect = 0\n",
    "\n",
    "for x,y in test_data:\n",
    "    pred = np.argmax(nn.feedforward(x))\n",
    "    true = np.argmax(y)\n",
    "    if pred == true:\n",
    "        correct+=1\n",
    "    else:\n",
    "        incorrect +=1\n",
    "        \n",
    "print(\"correct guesses:\",correct)\n",
    "print(\"incorrect guesses:\",incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "test_results = [(np.argmax(nn.feedforward(x)), np.argmax(y)) \n",
    "                        for (x, y) in test_data]\n",
    "tot = sum(int(x == y) for (x, y) in test_results)\n",
    "print (type(tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback(number,weights,biases):\n",
    "        num_gen = np.zeros(10)\n",
    "        num_gen[number] = 1\n",
    "        num_gen = np.reshape(num_gen,(10,1))\n",
    "        a = num_gen\n",
    "        for w,b in zip(weights[::-1] ,biases[::-1]):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        print(a.shape)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,30) and (10,1) not aligned: 30 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-59784f9bf10f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_im\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-25a9b88f2481>\u001b[0m in \u001b[0;36mfeedback\u001b[1;34m(number, weights, biases)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_gen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,30) and (10,1) not aligned: 30 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "test_im = feedback(1,nn.weights,nn.biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
