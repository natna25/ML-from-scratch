{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Filtering Spam Messages using a Naive Bayes ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to implement a Naive Bayes Machine learning algorithm with the goal of classifying text as spam or ham (not spam).\n",
    "\n",
    "The optimization strategy that will be implemented for this algorithm is Maximum Likelihood Estimation (MLE). \n",
    "\n",
    "#### Some ressources on Naive Bayes:\n",
    "\n",
    "https://www.youtube.com/watch?v=sjUDlJfdnKM : naive bayes\n",
    "\n",
    "https://www.youtube.com/watch?v=5NMxiOGL39M : how bayes theorem works\n",
    "\n",
    "https://towardsdatascience.com/spam-classifier-in-python-from-scratch-27a98ddd8e73 : spam classifier in python\n",
    "\n",
    "https://www.kaggle.com/astandrik/simple-spam-filter-using-naive-bayes\n",
    "\n",
    "https://www.kaggle.com/afagarap/gaussian-naive-bayes-classifier : gaussian bayes\n",
    "\n",
    "https://github.com/dannypaz/class/blob/262ae6eba4a97015e154bc9078328684ff8de97e/dsci-6003/deprecated/week1/1.4/DSCI6003-1.4.Lecture.ipynb : naive bayes course + MLE\n",
    "\n",
    "\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=27993555129092B2B08538AE1F67BDA5?doi=10.1.1.61.5542&rep=rep1&type=pdf : paper, diff naive bayes for spam classification\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes : scikit learn Naive bayes explanation\n",
    "\n",
    "\n",
    "https://web.stanford.edu/class/cs124/lec/naivebayes.pdf : stanford naive bayes course (good explanation for MLE)\n",
    "\n",
    "https://blog.metaflow.fr/ml-notes-why-the-log-likelihood-24f7b6c40f83 : article \"why use the log-likelyhood\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### 1) Divide the data in two groups: training and test examples.\n",
    "\n",
    "Importing the necessary libraries for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os #for going in the right directory to gather the data\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data in the text file we can identyfy its structure. The .txt file contains all of the features + target in plain text (ascii) format.\n",
    "\n",
    "The data is structured in the following manner:\n",
    "\n",
    "    - \"ham    words in the message \\n\" target and feature seperated by tab (\\t), samples seperated by carriage return \\n\n",
    "    - \"spam    words in the message\\n\"\n",
    "    \n",
    "we need to separate the features (text in the message) from the target (if it is a spam or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading file and seperating features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  target                                               text\n",
      "0    ham                  Yup i've finished c Ã¼ there...\\n\n",
      "1    ham             Remember to ask alex about his pizza\\n\n",
      "2    ham                     No da..today also i forgot..\\n\n",
      "3    ham  Ola would get back to you maybe not today but ...\n",
      "4    ham  Fwiw the reason I'm only around when it's time...\n"
     ]
    }
   ],
   "source": [
    "#file name and location\n",
    "file_name = \"messages.txt\"\n",
    "rel_path = \"data/\"\n",
    "\n",
    "\n",
    "data = pd.DataFrame([line.split(\"\\t\") for line in open(rel_path+file_name) ],columns = [\"target\",\"text\"])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data (optional)\n",
    "\n",
    "Indeed, we can see a lot of characters which should not be in the text data like '\\n' and some other special characters whic were not properly decoded.\n",
    "\n",
    "1) First we remove the \"\\n\" from the text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  target                                               text\n",
      "0    ham                  Yup i've finished c Ã¼ there...\\n\n",
      "1    ham             Remember to ask alex about his pizza\\n\n",
      "2    ham                     No da..today also i forgot..\\n\n",
      "3    ham  Ola would get back to you maybe not today but ...\n",
      "4    ham  Fwiw the reason I'm only around when it's time...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yup i've finished c Ã¼ there...\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"] = data[\"text\"][:-1] #not working....\n",
    "\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "sample = data[\"text\"][0][:-1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) find the rows with missing data and just enter a random word, this error came up for the very last line which contained the word \"ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "for i,mess in enumerate(data[\"text\"]):\n",
    "    #print(i)\n",
    "    if type(mess) == float:\n",
    "        print(mess)\n",
    "        data.iloc[i,1] = \"Ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) (optional) Remove some common words which dont have any sort of indication wether or not a word could contribute to the message being a spam. Since we consider all these features to be independent from one another, it doesnt matter if we remove words that would take out the 'meaning' of the text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Turning the target from text to a binary (integer) value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.target == \"ham\", 'y'] = 0\n",
    "data.loc[data.target == \"spam\", 'y'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">target</th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4328</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>4328</td>\n",
       "      <td>4328</td>\n",
       "      <td>4072</td>\n",
       "      <td>Sorry, I'll call later\\n</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>672</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>596</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target                     text         \\\n",
       "     count unique   top  freq count unique   \n",
       "y                                            \n",
       "0.0   4328      1   ham  4328  4328   4072   \n",
       "1.0    672      1  spam   672   672    596   \n",
       "\n",
       "                                                             \n",
       "                                                   top freq  \n",
       "y                                                            \n",
       "0.0                           Sorry, I'll call later\\n   26  \n",
       "1.0  Please call our customer service representativ...    4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"y\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the 5000 samples in this dataset:\n",
    "- 87% are not spam\n",
    "- 13% are spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000002A6D0C823C8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000002A6D0D7FCF8>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ploting the length of messages for both spam and ham\n",
    "data[\"message_length\"] = data[\"text\"].apply(len)\n",
    "\n",
    "print(data[\"text\"].dtype)\n",
    "\n",
    "data.hist(by=\"target\",column='message_length') #histograms of the target classes based on the message length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also notice that spams tend to have more characters than hams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting training and test data\n",
    "\n",
    "Now we can proceed to partition our data into 2 sets: \n",
    "\n",
    "- one training set with a majority of the samples (usualy > 60%) that we will use to train our model on\n",
    "\n",
    "- And a test set with the remaining samples so that we can validate the performance of our model on unseen data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples 3500\n",
      "testing samples 1500\n"
     ]
    }
   ],
   "source": [
    "X = data[\"text\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "train_size = 0.7 \n",
    "train_samples = round(X.size * train_size) # 3500 samples (with 0.7)\n",
    "\n",
    "X_train = X[:train_samples].reset_index(drop=True) # we need to reset the indexes of the data to make it easier to access later\n",
    "y_train = y[:train_samples].reset_index(drop=True)\n",
    "\n",
    "print(\"training samples\" ,X_train.size)\n",
    "\n",
    "X_test = X[train_samples:].reset_index(drop=True)\n",
    "y_test = y[train_samples:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"testing samples\" ,X_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better way to split\n",
    "Here we are simply taking the first n% and spliting it into two parts. A better way to split would be to \"shuffle\" the data by picking from a uniform distribution if we get a value over our desired threshold. \n",
    "\n",
    "This way is better because there could be some unwanted corelation in the way our data was collected and stored in the file (ex: hams in the begining and spams at the end). This is why shufling the data makes more sense to provide a better statistical distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Fwiw the reason I'm only around when it's time...\n",
      "1                                             Yup...\\n\n",
      "2    U wake up already? Wat u doing? U picking us u...\n",
      "3    Bognor it is! Should be splendid at this time ...\n",
      "4                             Sorry, I'll call later\\n\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "threshold = train_size\n",
    "samples = data[\"text\"].size\n",
    "\n",
    "train_i = list()\n",
    "test_i = list()\n",
    "\n",
    "for index in range(X.size):\n",
    "    if np.random.uniform(0,1) < threshold:\n",
    "        train_i += [index]\n",
    "    else:\n",
    "        test_i += [index]\n",
    "\n",
    "X_train_norm =  X.loc[train_i].reset_index(drop=True)\n",
    "y_train_norm =  y.loc[train_i].reset_index(drop=True)\n",
    "\n",
    "X_test_norm = X.loc[test_i].reset_index(drop=True)\n",
    "y_test_norm = y.loc[test_i].reset_index(drop=True)\n",
    "\n",
    "print(X_test_norm.head())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary generation\n",
    "\n",
    "Now that we have cleaned our data, split it into the training and testing groups we can build up our dictionary. This dictionary will contain all of the words that appear in the training set.\n",
    "\n",
    "The dimension of this dictionary will dictate the dimensions of our feature vector\n",
    "* We only take the 3000 most common occuring words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['call', 245], ['get', 209], ['ur', 190], ['You', 156], ['go', 143], ['like', 139], ['know', 129], ['got', 127], ['come', 120], ['time', 104], ['want', 95], ['Call', 95], ['If', 91], ['going', 91], ['How', 89], ['still', 88], ['send', 85], ['need', 85], ['So', 83], ['one', 83], ['text', 81], ['No', 80], ['But', 78], ['We', 78], ['see', 73], ['back', 72], ['home', 71], ['good', 71], ['tell', 70], ['mobile', 70], ['Do', 69], ['Just', 69], ['free', 68], ['Ok', 66], ['My', 65], ['take', 64], ['What', 64], ['love', 63], ['And', 63], ['FREE', 62], ['new', 60], ['think', 60], ['The', 59], ['dont', 59], ['give', 58], ['day', 57], ['Have', 56], ['Your', 56], ['Hey', 56], ['da', 54], ['later', 52], ['great', 52], ['much', 52], ['To', 51], ['make', 51], ['phone', 50], ['Are', 49], ['This', 47], ['reply', 47], ['ask', 46], ['number', 46], ['meet', 46], ['Its', 45], ['claim', 45], ['way', 44], ['Can', 43], ['last', 43], ['right', 43], ['contact', 43], ['gonna', 42], ['really', 42], ['Please', 42], ['txt', 42], ['Good', 42], ['would', 41], ['week', 41], ['say', 41], ['Pls', 40], ['sent', 40], ['pick', 40], ['stop', 40], ['Txt', 39], ['Reply', 39], ['next', 38], ['work', 38], ['per', 38], ['Hi', 38], ['cos', 38], ['night', 38], ['said', 37], ['Oh', 37], ['find', 37], ['It', 37], ['told', 36], ['cant', 36], ['could', 36], ['miss', 35], ['anything', 35], ['keep', 35], ['He', 35], ['tomorrow', 35], ['hope', 34], ['message', 34], ['let', 34], ['every', 34], ['Not', 34], ['Happy', 34], ['around', 33], ['wan', 33], ['She', 33], ['Sorry', 33], ['us', 32], ['many', 32], ['Ãœ', 32], ['Yeah', 32], ['Did', 32], ['Is', 32], ['today', 31], ['went', 31], ['please', 31], ['ok', 31], ['buy', 31], ['Then', 30], ['coming', 30], ['Text', 30], ['service', 30], ['feel', 30], ['Hope', 30], ['cash', 30], ['always', 30], ['sure', 30], ['place', 30], ['Ur', 30], ['STOP', 29], ['Get', 29], ['leave', 29], ['Lol', 29], ['thing', 29], ['people', 28], ['awarded', 28], ['dun', 28], ['As', 28], ['already', 28], ['try', 28], ['also', 27], ['Or', 27], ['Nokia', 27], ['trying', 27], ['Gud', 27], ['first', 27], ['Yup', 26], ['care', 26], ['bit', 26], ['Thanks', 26], ['prize', 26], ['When', 26], ['something', 26], ['even', 26], ['In', 25], ['Yes', 25], ['use', 25], ['msg', 25], ['Where', 25], ['Send', 25], ['finish', 25], ['Dear', 25], ['thought', 24], ['Will', 24], ['That', 24], ['im', 24], ['money', 23], ['wait', 23], ['YOU', 23], ['morning', 23], ['well', 23], ['pls', 23], ['They', 23], ['Now', 23], ['life', 23], ['wat', 23], ['meeting', 23], ['Free', 23], ['New', 23], ['name', 23], ['someone', 22], ['might', 22], ['sleep', 22], ['never', 22], ['Mobile', 22], ['happy', 22], ['hi', 22], ['lot', 22], ['draw', 22], ['dear', 22], ['THE', 22], ['things', 22], ['friends', 21], ['says', 21], ['talk', 21], ['lunch', 21], ['called', 21], ['shows', 21], ['wish', 21], ['customer', 21], ['Well', 21], ['help', 21], ['without', 21], ['waiting', 21], ['Love', 21], ['For', 20], ['Wat', 20], ['win', 20], ['hear', 20], ['big', 20], ['latest', 20], ['Only', 20], ['car', 20], ['See', 20], ['face', 20], ['IS', 20], ['getting', 19], ['nice', 19], ['All', 19], ['best', 19], ['wif', 19], ['special', 19], ['late', 19], ['friend', 19], ['asked', 19], ['Claim', 19], ['left', 19], ['another', 19], ['dat', 19], ['stuff', 19], ['UR', 19], ['soon', 18], ['part', 18], ['ill', 18], ['eat', 18], ['mins', 18], ['Why', 18], ['real', 18], ['CALL', 18], ['PO', 18], ['long', 18], ['didnt', 18], ['receive', 18], ['made', 17], ['MY', 17], ['guys', 17], ['babe', 17], ['quite', 17], ['sorry', 17], ['plan', 17], ['look', 17], ['looking', 17], ['person', 17], ['half', 17], ['little', 17], ['ever', 17], ['ready', 17], ['live', 17], ['ME', 17], ['network', 17], ['check', 16], ['tonight', 16], ['probably', 16], ['thk', 16], ['play', 16], ['thats', 16], ['better', 16], ['wont', 16], ['tried', 16], ['mean', 16], ['watch', 16], ['end', 16], ['since', 16], ['chance', 16], ['minutes', 16], ['Tell', 16], ['shopping', 16], ['tot', 16], ['hour', 16], ['chat', 16], ['selected', 16], ['abt', 15], ['LOVE', 15], ['put', 15], ['pay', 15], ['dis', 15], ['year', 15], ['came', 15], ['shit', 15], ['able', 15], ['bad', 15], ['guaranteed', 15], ['Valid', 15], ['hav', 15], ['camera', 15], ['havent', 15], ['two', 15], ['bring', 15], ['smile', 15], ['Had', 15], ['yeah', 15], ['min', 15], ['heart', 15], ['NOW', 15], ['wants', 14], ['wake', 14], ['Got', 14], ['entry', 14], ['collect', 14], ['video', 14], ['enough', 14], ['days', 14], ['details', 14], ['Am', 14], ['dad', 14], ['bus', 14], ['forgot', 14], ['working', 14], ['Dont', 14], ['colour', 14], ['guess', 14], ['may', 14], ['Im', 14], ['school', 14], ['saw', 14], ['done', 14], ['remember', 14], ['IM', 14], ['fun', 14], ['finished', 13], ['ya', 13], ['Should', 13], ['texts', 13], ['word', 13], ['goes', 13], ['together', 13], ['job', 13], ['enjoy', 13], ['haf', 13], ['start', 13], ['walk', 13], ['calling', 13], ['IN', 13], ['Want', 13], ['tone', 13], ['ON', 13], ['Haha', 13], ['holiday', 13], ['must', 13], ['watching', 13], ['away', 13], ['class', 13], ['saying', 13], ['actually', 13], ['pounds', 13], ['line', 13], ['Take', 13], ['juz', 13], ['means', 13], ['wil', 13], ['takes', 12], ['till', 12], ['weekly', 12], ['reach', 12], ['drive', 12], ['house', 12], ['cause', 12], ['Still', 12], ['wanna', 12], ['attempt', 12], ['yet', 12], ['land', 12], ['man', 12], ['Holiday', 12], ['await', 12], ['missed', 12], ['calls', 12], ['Update', 12], ['dinner', 12], ['full', 12], ['Today', 12], ['lor', 12], ['Double', 12], ['Our', 12], ['guy', 12], ['Cos', 12], ['trip', 12], ['problem', 12], ['wanted', 12], ['weekend', 12], ['AT', 12], ['set', 12], ['nothing', 12], ['jus', 12], ['town', 12], ['maybe', 11], ['taking', 11], ['smth', 11], ['Me', 11], ['hold', 11], ['join', 11], ['food', 11], ['sch', 11], ['mind', 11], ['telling', 11], ['order', 11], ['UP', 11], ['GO', 11], ['gift', 11], ['everything', 11], ['run', 11], ['Was', 11], ['early', 11], ['post', 11], ['sexy', 11], ['comes', 11], ['Orange', 11], ['angry', 11], ['room', 11], ['birthday', 11], ['knw', 11], ['den', 11], ['boy', 11], ['Last', 11], ['TO', 11], ['Box', 11], ['bed', 11], ['til', 11], ['driving', 11], ['started', 11], ['important', 11], ['Thats', 11], ['believe', 11], ['THIS', 11], ['Customer', 11], ['ringtone', 11], ['test', 11], ['open', 11], ['though', 11], ['cool', 11], ['sms', 11], ['Expires', 11], ['thanks', 11], ['visit', 10], ['office', 10], ['WIN', 10], ['words', 10], ['wid', 10], ['After', 10], ['times', 10], ['At', 10], ['WON', 10], ['Bonus', 10], ['delivery', 10], ['took', 10], ['Todays', 10], ['Ya', 10], ['seen', 10], ['dating', 10], ['support', 10], ['family', 10], ['makes', 10], ['Let', 10], ['show', 10], ['sis', 10], ['apply', 10], ['hours', 10], ['pain', 10], ['hurt', 10], ['Maybe', 10], ['plz', 10], ['sweet', 10], ['code', 10], ['found', 10], ['book', 10], ['valued', 10], ['fine', 10], ['month', 10], ['pretty', 10], ['wit', 10], ['Identifier', 10], ['Account', 10], ['Statement', 10], ['Smile', 10], ['direct', 9], ['NOKIA', 9], ['old', 9], ['Dunno', 9], ['WITH', 9], ['gone', 9], ['Stop', 9], ['Going', 9], ['From', 9], ['xxx', 9], ['neva', 9], ['FOR', 9]]\n"
     ]
    }
   ],
   "source": [
    "def create_dict(size,data):\n",
    "    \n",
    "    \n",
    "    all_words = []\n",
    "    for i,message in data.iteritems():\n",
    "        words = message.split() #splits line into a list of words\n",
    "        for word in words:\n",
    "            if word.isalpha() and len(word) > 1 and word not in stopwords.words('english')  : #check if word is alpha (letters) and more than one character in length\n",
    "                all_words += [word] \n",
    "        \n",
    "    # now we can count the occurence of each word in the list\n",
    "    dictionary = Counter(all_words).most_common(size)\n",
    "    dictionary = [list(word) for word in dictionary] #transforming dictionary into list format\n",
    "    return dictionary\n",
    "\n",
    "dictionary = create_dict(500,X_train)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to take all of the words in lowecase but spams usually contain a lot of uppecase words, so maybe this was not the optimal choice or it might not have any incidence on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction \n",
    "\n",
    "Now we extract the features from the text data and generate the proper vectors based on our dictionary \n",
    "This function will cycle through all messages and extract the proper features that appear in the dictionary in order to create the appropriate feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 500)\n"
     ]
    }
   ],
   "source": [
    "def extract_features(data,dictionary):\n",
    "    feature_vec = np.zeros(shape=(len(data) , len(dictionary))) #creates our matrix (all our feature vectors but together) filled with zeroes, we fill in the 1 when the word occurs in the dictionary\n",
    "    print(feature_vec.shape)\n",
    "    for i,message in data.iteritems():\n",
    "        words = message.split()\n",
    "        for word in words:\n",
    "            #print(word)\n",
    "            for j,di in enumerate(dictionary):\n",
    "                if di[0] == word:\n",
    "                    feature_vec[i][j] = 1\n",
    "    return feature_vec\n",
    "\n",
    "feature_vec = extract_features(X_train,dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 209]\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "feat_num = 1\n",
    "print(dictionary[feat_num])\n",
    "appear = 0\n",
    "for vec in feature_vec:\n",
    "    if vec[feat_num] == 1:\n",
    "        appear+=1\n",
    "print(appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature vector matrix corresponds to the occurence of words in a given message that belong to the dictionary. Since we are only doing binomial (bernoulli) bayes, we dont put any emphasis on counting the occurence of these words in the message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Implementation\n",
    "\n",
    "### Now we are going into the difficult part of the implementation. We need to implement the functions fit() and predict() which we are used to handling without having to worry about the code behind it.\n",
    "\n",
    "Bayes theorem tells us that we can calulate the probability of a certain event A, given that an event B is already true. This conditional probability allows us to refine the guesses that we make on a certain event as long as event B is linked to the realisation of event A.\n",
    "\n",
    "In our case, we wish to calculate the probability that a given text message is a spam or not. We can condition the probability of this event base on the words that are present in the message itself. Indeed, certain words are more present in spam messages than they are in normal messages. We can take advantage of this fact to refine our prediction that a certain message is a spam.\n",
    "\n",
    "The most basic probabilistic approach to take would be to gather huge amounts of data and compare the ratio of spam to non spam messages. Say that out of 1 million messages, only 1% are spam. Then, the probability of a new message being a spam would be of 1%. \n",
    "However, this is an extremely bad approach as it would lead to numerous false positives and false negatives. Furthermore, this guess is only based on past data and the amounts of it that we had.\n",
    "\n",
    "This is to say that we need to find a different approach to classify wether a given message is a spam or not; and we do this by using Bayes' theorem and conditional probability.\n",
    "\n",
    "### A bit of math: Bayes Theorem\n",
    "\n",
    "There are a few ways of expressing this theorem. But first, lets list a bit of terminology:\n",
    "\n",
    "$$posterior = \\dfrac{prior \\times likelihood}{evidence}$$\n",
    "\n",
    "1) Posterior: $ P(A|B) $\n",
    "\n",
    "2) Likelyhood: $ P(B|A) $\n",
    "\n",
    "3) Prior : $ P(A) $\n",
    "\n",
    "4) Posterior: $ P(B) $\n",
    "\n",
    "This gives us the formula:\n",
    "\n",
    "$$ P(A|B)= \\dfrac{P(B|A) \\times P(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "\n",
    "#### Our implementation:\n",
    "\n",
    "In our case, we wish to use bayes theorem to allow for a better prediction of wether or not a certain text message is a spam or not. \n",
    "\n",
    "Let y be our target: wether an email is a spam or not (we only have one class which can take 2 values -> bernoulli/binomial)\n",
    "let X be our features: the words present in our message that also appear in the dictionary.\n",
    "\n",
    "Therefore, we want to know the probability:\n",
    "\n",
    "$$ P(y | X_{n}) $$\n",
    "\n",
    "If we also consider that our features are i.i.d (independently identically distributed) then we can write:\n",
    "\n",
    "$$  P(y | X_{n}) = P(y | X_{0}) \\times ... \\times P(y| X_{N}) = \\prod_{n=0}^N P(y| X_n) $$\n",
    "\n",
    "In other words, we are calculating the probability of a message being a spam, given the occurence of certain words in that message.\n",
    "\n",
    "\n",
    "### Defining our Machine Learning model:\n",
    "\n",
    "now that we have gotten a bit of the math out of the way let's continue to define how we are going to implement all this theory into python code and apply it to the data which we prepared earlier.\n",
    "\n",
    "#### Hyperparameters:\n",
    "Contrary to the models parameters $\\theta$, hyperparameters are more general to our implementation rather than model parameters. We define the follwoing 2 metrics\n",
    "\n",
    "1) N = number of input variables, also called features\n",
    "\n",
    "2) I = number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "Our model will implement a naive bayes bernoulli classifier which is optimized using MLE (maximum likelyhood estimation). The first part in developping this model is to train it, This training function will go through our training data in order to find the parameters that give us the best prediction. \n",
    "\n",
    "\n",
    "### building the conditional probability table:\n",
    "\n",
    "our model is simply a collection of parameters that will be helpfull later to make a prediction. There are 3 parameters that we have to find in order to fill in our model.\n",
    "\n",
    "1) $\\phi_y = \\frac{\\sum^I_{i=1} \\| \\{ y=1\\} } {I} $  Likelyhood of a given message being a spam, number of spams/nb of messages\n",
    "\n",
    "2) $ \\phi_{n|y=1} = \\frac{\\sum^I_{i=1} \\| \\{X_n=1,y=1\\} } { \\sum^I_{i=1} \\| \\{y=1\\} } $ likelyhood of feature (Xn) appearing in a spam\n",
    "\n",
    "3) $ \\phi_{n|y=0} = \\frac{\\sum^I_{i=1} \\| \\{X_n=1,y=0\\} } { \\sum^I_{i=1} \\| \\{y=0\\} } $ likelyhood of feature (Xn) appearing in a ham\n",
    "\n",
    "In order to fill this table, we just need to go through all of the training data, tally up the occurences of certain features (words present in the dictionary) for both spams and hams. Then we apply the formulas given above to get the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cond_prob_table(train_data,train_labels,dictionary):\n",
    "    prob_table = pd.DataFrame(columns=[\"words\",\"total_occurence\",\"spam_occurence\",\"ham_occurence\",\"spam_cond_proba\",\"ham_cond_proba\",\"spam_proba\"])\n",
    "    prob_table[\"words\"] = [word[0] for word in dictionary]\n",
    "    prob_table[\"total_occurence\"] = [word[1] for word in dictionary]\n",
    "    prob_table[\"spam_occurence\"]= prob_table[\"spam_occurence\"].fillna(0)\n",
    "    prob_table[\"ham_occurence\"]= prob_table[\"ham_occurence\"].fillna(0)\n",
    "    \n",
    "    \n",
    "    total = len(train_labels)\n",
    "    total_spams = train_labels.sum()\n",
    "    total_hams = total - total_spams \n",
    "    \n",
    "    prob_table[\"spam_proba\"] = (total_spams+1)/(total+2)\n",
    "    \n",
    "    #print(prob_table.head())\n",
    "  \n",
    "    #we need to loop over all of the training data\n",
    "    for i,feature_vec in enumerate(train_data):\n",
    "        #print(feature_vec, train_labels[i] ,i)\n",
    "        for j,feature in enumerate(feature_vec):\n",
    "            if feature == 1: #if a feature appears \n",
    "                if train_labels[i] == 1: #if it is a spam\n",
    "                    prob_table.iloc[j,2] += 1\n",
    "                 #increment the occurence of this feature in the spam column\n",
    "                else:\n",
    "                    prob_table.iloc[j,3] += 1 #increment the occurence of this feature in the ham column\n",
    "\n",
    "    #calculate the conditional probability that a message is a spam, conditioned on a feature being present                \n",
    "    \n",
    "    \n",
    "    prob_table[\"spam_cond_proba\"] = [(spam_occur+1)/(total_spams+2) for spam_occur in prob_table[\"spam_occurence\"]]\n",
    "    prob_table[\"ham_cond_proba\"] = [(ham_occur+1)/(total_hams+2) for ham_occur in prob_table[\"ham_occurence\"]]\n",
    "    \n",
    "\n",
    "    #print(prob_table.head())\n",
    "    return prob_table\n",
    "    \n",
    "#begin_model = build_cond_prob_table(feature_vec,y_train,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train,y_train,dictionary):\n",
    "    prob_table = pd.DataFrame(columns=[\"words\",\"total_occurence\",\"spam_occurence\",\"ham_occurence\",\"spam_cond_proba\",\"ham_cond_proba\",\"spam_proba\"])\n",
    "    prob_table[\"words\"] = [word[0] for word in dictionary]\n",
    "    prob_table[\"total_occurence\"] = [word[1] for word in dictionary]\n",
    "    prob_table[\"spam_occurence\"]= prob_table[\"spam_occurence\"].fillna(0)\n",
    "    prob_table[\"ham_occurence\"]= prob_table[\"ham_occurence\"].fillna(0)\n",
    "    \n",
    "    \n",
    "    total = len(y_train)\n",
    "    total_spams = y_train.sum()\n",
    "    total_hams = total - total_spams \n",
    "    \n",
    "    prob_table[\"spam_proba\"] = (total_spams+1)/(total+2) #phi_y , parameter 1\n",
    "    \n",
    "    \n",
    "    for i,message in enumerate(X_train):\n",
    "        for word in message.split():\n",
    "            for n,dic_word in enumerate(dictionary):\n",
    "                if word == dic_word[0]:\n",
    "                    if y_train[i] == 1:\n",
    "                        prob_table.iloc[n,2] += 1\n",
    "                    else:\n",
    "                        prob_table.iloc[n,3] += 1\n",
    "                        \n",
    "                        \n",
    "    prob_table[\"spam_cond_proba\"] = [(spam_occur+1)/(total_spams+2) for spam_occur in prob_table[\"spam_occurence\"]]\n",
    "    prob_table[\"ham_cond_proba\"] = [(ham_occur+1)/(total_hams+2) for ham_occur in prob_table[\"ham_occurence\"]]\n",
    "    \n",
    "    return prob_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_model = fit(X_train,y_train,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>total_occurence</th>\n",
       "      <th>spam_occurence</th>\n",
       "      <th>ham_occurence</th>\n",
       "      <th>spam_cond_proba</th>\n",
       "      <th>ham_cond_proba</th>\n",
       "      <th>spam_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>call</td>\n",
       "      <td>245</td>\n",
       "      <td>122</td>\n",
       "      <td>123</td>\n",
       "      <td>0.262821</td>\n",
       "      <td>0.040843</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get</td>\n",
       "      <td>209</td>\n",
       "      <td>27</td>\n",
       "      <td>182</td>\n",
       "      <td>0.059829</td>\n",
       "      <td>0.060277</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ur</td>\n",
       "      <td>190</td>\n",
       "      <td>57</td>\n",
       "      <td>133</td>\n",
       "      <td>0.123932</td>\n",
       "      <td>0.044137</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You</td>\n",
       "      <td>156</td>\n",
       "      <td>48</td>\n",
       "      <td>108</td>\n",
       "      <td>0.104701</td>\n",
       "      <td>0.035903</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>go</td>\n",
       "      <td>143</td>\n",
       "      <td>9</td>\n",
       "      <td>134</td>\n",
       "      <td>0.021368</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>like</td>\n",
       "      <td>139</td>\n",
       "      <td>8</td>\n",
       "      <td>131</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>know</td>\n",
       "      <td>129</td>\n",
       "      <td>9</td>\n",
       "      <td>120</td>\n",
       "      <td>0.021368</td>\n",
       "      <td>0.039855</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>got</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.041502</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>come</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>time</td>\n",
       "      <td>104</td>\n",
       "      <td>11</td>\n",
       "      <td>93</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.030962</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>want</td>\n",
       "      <td>95</td>\n",
       "      <td>11</td>\n",
       "      <td>84</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.027997</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Call</td>\n",
       "      <td>95</td>\n",
       "      <td>83</td>\n",
       "      <td>12</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>If</td>\n",
       "      <td>91</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.026680</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>going</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.029974</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>87</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>still</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.028327</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>send</td>\n",
       "      <td>85</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>0.061966</td>\n",
       "      <td>0.019104</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>need</td>\n",
       "      <td>85</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.027339</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>So</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.027339</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>one</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.026680</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>text</td>\n",
       "      <td>81</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.012846</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>No</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.024045</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>But</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We</td>\n",
       "      <td>78</td>\n",
       "      <td>23</td>\n",
       "      <td>55</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.018445</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>see</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>69</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>back</td>\n",
       "      <td>72</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.020422</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>home</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>good</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tell</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mobile</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>0.132479</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>hours</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>pain</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>hurt</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Maybe</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>plz</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>sweet</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>code</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>found</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>book</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>valued</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>fine</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>month</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>pretty</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>wit</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Identifier</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Account</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Statement</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Smile</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>NOKIA</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021368</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>old</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Dunno</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>WITH</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>gone</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Stop</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Going</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>From</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>xxx</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>neva</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>FOR</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.133352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  total_occurence  spam_occurence  ham_occurence  \\\n",
       "0          call              245             122            123   \n",
       "1           get              209              27            182   \n",
       "2            ur              190              57            133   \n",
       "3           You              156              48            108   \n",
       "4            go              143               9            134   \n",
       "5          like              139               8            131   \n",
       "6          know              129               9            120   \n",
       "7           got              127               2            125   \n",
       "8          come              120               1            119   \n",
       "9          time              104              11             93   \n",
       "10         want               95              11             84   \n",
       "11         Call               95              83             12   \n",
       "12           If               91              11             80   \n",
       "13        going               91               1             90   \n",
       "14          How               89               2             87   \n",
       "15        still               88               3             85   \n",
       "16         send               85              28             57   \n",
       "17         need               85               3             82   \n",
       "18           So               83               1             82   \n",
       "19          one               83               3             80   \n",
       "20         text               81              43             38   \n",
       "21           No               80               8             72   \n",
       "22          But               78               0             78   \n",
       "23           We               78              23             55   \n",
       "24          see               73               4             69   \n",
       "25         back               72              11             61   \n",
       "26         home               71               1             70   \n",
       "27         good               71               2             69   \n",
       "28         tell               70               8             62   \n",
       "29       mobile               70              61              9   \n",
       "..          ...              ...             ...            ...   \n",
       "470       hours               10               4              6   \n",
       "471        pain               10               0             10   \n",
       "472        hurt               10               0             10   \n",
       "473       Maybe               10               0             10   \n",
       "474         plz               10               0             10   \n",
       "475       sweet               10               0             10   \n",
       "476        code               10              10              0   \n",
       "477       found               10               1              9   \n",
       "478        book               10               2              8   \n",
       "479      valued               10              10              0   \n",
       "480        fine               10               0             10   \n",
       "481       month               10               1              9   \n",
       "482      pretty               10               0             10   \n",
       "483         wit               10               0             10   \n",
       "484  Identifier               10              10              0   \n",
       "485     Account               10              10              0   \n",
       "486   Statement               10              10              0   \n",
       "487       Smile               10               0             10   \n",
       "488      direct                9               7              2   \n",
       "489       NOKIA                9               9              0   \n",
       "490         old                9               0              9   \n",
       "491       Dunno                9               0              9   \n",
       "492        WITH                9               0              9   \n",
       "493        gone                9               0              9   \n",
       "494        Stop                9               5              4   \n",
       "495       Going                9               0              9   \n",
       "496        From                9               6              3   \n",
       "497         xxx                9               2              7   \n",
       "498        neva                9               0              9   \n",
       "499         FOR                9               6              3   \n",
       "\n",
       "     spam_cond_proba  ham_cond_proba  spam_proba  \n",
       "0           0.262821        0.040843    0.133352  \n",
       "1           0.059829        0.060277    0.133352  \n",
       "2           0.123932        0.044137    0.133352  \n",
       "3           0.104701        0.035903    0.133352  \n",
       "4           0.021368        0.044466    0.133352  \n",
       "5           0.019231        0.043478    0.133352  \n",
       "6           0.021368        0.039855    0.133352  \n",
       "7           0.006410        0.041502    0.133352  \n",
       "8           0.004274        0.039526    0.133352  \n",
       "9           0.025641        0.030962    0.133352  \n",
       "10          0.025641        0.027997    0.133352  \n",
       "11          0.179487        0.004282    0.133352  \n",
       "12          0.025641        0.026680    0.133352  \n",
       "13          0.004274        0.029974    0.133352  \n",
       "14          0.006410        0.028986    0.133352  \n",
       "15          0.008547        0.028327    0.133352  \n",
       "16          0.061966        0.019104    0.133352  \n",
       "17          0.008547        0.027339    0.133352  \n",
       "18          0.004274        0.027339    0.133352  \n",
       "19          0.008547        0.026680    0.133352  \n",
       "20          0.094017        0.012846    0.133352  \n",
       "21          0.019231        0.024045    0.133352  \n",
       "22          0.002137        0.026021    0.133352  \n",
       "23          0.051282        0.018445    0.133352  \n",
       "24          0.010684        0.023057    0.133352  \n",
       "25          0.025641        0.020422    0.133352  \n",
       "26          0.004274        0.023386    0.133352  \n",
       "27          0.006410        0.023057    0.133352  \n",
       "28          0.019231        0.020751    0.133352  \n",
       "29          0.132479        0.003294    0.133352  \n",
       "..               ...             ...         ...  \n",
       "470         0.010684        0.002306    0.133352  \n",
       "471         0.002137        0.003623    0.133352  \n",
       "472         0.002137        0.003623    0.133352  \n",
       "473         0.002137        0.003623    0.133352  \n",
       "474         0.002137        0.003623    0.133352  \n",
       "475         0.002137        0.003623    0.133352  \n",
       "476         0.023504        0.000329    0.133352  \n",
       "477         0.004274        0.003294    0.133352  \n",
       "478         0.006410        0.002964    0.133352  \n",
       "479         0.023504        0.000329    0.133352  \n",
       "480         0.002137        0.003623    0.133352  \n",
       "481         0.004274        0.003294    0.133352  \n",
       "482         0.002137        0.003623    0.133352  \n",
       "483         0.002137        0.003623    0.133352  \n",
       "484         0.023504        0.000329    0.133352  \n",
       "485         0.023504        0.000329    0.133352  \n",
       "486         0.023504        0.000329    0.133352  \n",
       "487         0.002137        0.003623    0.133352  \n",
       "488         0.017094        0.000988    0.133352  \n",
       "489         0.021368        0.000329    0.133352  \n",
       "490         0.002137        0.003294    0.133352  \n",
       "491         0.002137        0.003294    0.133352  \n",
       "492         0.002137        0.003294    0.133352  \n",
       "493         0.002137        0.003294    0.133352  \n",
       "494         0.012821        0.001647    0.133352  \n",
       "495         0.002137        0.003294    0.133352  \n",
       "496         0.014957        0.001318    0.133352  \n",
       "497         0.006410        0.002635    0.133352  \n",
       "498         0.002137        0.003294    0.133352  \n",
       "499         0.014957        0.001318    0.133352  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "The predictions that we make, will of course be based on our model paremeters. Once our model has been fit on the training data we simply need to apply Bayes rule and to choose the class prediction with the highest probability (or likelyhood if you like).\n",
    "\n",
    "The formula for making a prediction is very simple. We will calculate a probability that a given message belongs in each possible class, given all of its features. Then we just take the probability with the highest value amongst these. This is expressed mathematically as:\n",
    "\n",
    "$$ P(Y|X) = \\frac{P(y)\\times P(X|y)} {P(X)} $$\n",
    "\n",
    "$$ P(Y|X) = \\frac{P(y)\\times \\prod_{n=1}^N P(X_n|y)} {P(X)} $$\n",
    "\n",
    "We simplify this down to :\n",
    "\n",
    "$$ P(Y|X) = P(y)\\times \\prod_{n=1}^N P(X_n|y) $$\n",
    "\n",
    "Then, its all a matter of taking the max from these predictions\n",
    "\n",
    "$$ Prediction = argmax_y P(Y|X) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(feature_vect,label,model):\n",
    "    pred_spam = model[\"spam_proba\"].iloc[0] #phi y= 1\n",
    "    pred_ham = 1 - pred_spam\n",
    "    for n,feature in enumerate(feature_vect):\n",
    "        if feature == 1:\n",
    "            pred_spam *= model[\"spam_cond_proba\"].iloc[n]\n",
    "            pred_ham *= model[\"ham_cond_proba\"].iloc[n]\n",
    "        \n",
    "    \n",
    "    #print(\"prediction message is spam\", pred_spam, \"prediction for ham\",pred_ham, \" model is really\", label)\n",
    "    \n",
    "    if pred_spam > pred_ham:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_proba(feature_vect,label,model):\n",
    "    import math\n",
    "    \n",
    "    pred_spam = math.log(model[\"spam_proba\"].iloc[0])\n",
    "    pred_ham = math.log(1 - model[\"spam_proba\"].iloc[0])\n",
    "    for n,feature in enumerate(feature_vect):\n",
    "        if feature:\n",
    "            pred_spam += math.log(model[\"spam_cond_proba\"].iloc[n])\n",
    "            pred_ham += math.log(model[\"ham_cond_proba\"].iloc[n])\n",
    "        \n",
    "    \n",
    "    #print(\"prediction message is spam\", pred_spam, \"prediction for ham\",pred_ham, \" model is really\", label)\n",
    "    \n",
    "    if pred_spam > pred_ham:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 500)\n",
      "Confusion matrix, out of 1500 samples, with: 206.0 spams and: 1294.0 hams\n",
      "true positive: 190 , 92.23300970873787 %\n",
      "true negative: 1229 , 94.97681607418856 %\n",
      "false positive: 65 , 5.023183925811438 %\n",
      "false negative: 16 , 7.766990291262135 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.95      0.97      1294\n",
      "         1.0       0.75      0.92      0.82       206\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1500\n",
      "   macro avg       0.87      0.94      0.90      1500\n",
      "weighted avg       0.95      0.95      0.95      1500\n",
      "\n",
      "[[1229   65]\n",
      " [  16  190]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "\n",
    "feature_vec_test = extract_features(X_test,dictionary)\n",
    "\n",
    "total_samples = len(y_test)\n",
    "total_spams = y_test.sum()\n",
    "total_hams = total_samples - total_spams\n",
    "\n",
    "true_pos  = 0 # correctly predicted spam\n",
    "true_neg  = 0 # correctly predicted ham\n",
    "false_pos = 0 # ham predicted as spam\n",
    "false_neg = 0 # spam predicted as ham\n",
    "\n",
    "pred_label = np.zeros(shape=len(y_test))\n",
    "\n",
    "for i,sample in enumerate(feature_vec_test):\n",
    "    real_label = y_test[i]\n",
    "    pred_label[i] = predict_log_proba(sample,real_label,begin_model)\n",
    "    \n",
    "    if pred_label[i] == 1 and real_label == 1: # true positive\n",
    "        true_pos +=1\n",
    "    elif pred_label[i] == 0 and real_label == 0: #true negative\n",
    "        true_neg += 1\n",
    "    elif pred_label[i] == 1 and real_label == 0: #false positive\n",
    "        #print(X_test[i])\n",
    "        false_pos += 1\n",
    "    elif pred_label[i] == 0 and real_label == 1: #false negative\n",
    "        false_neg +=1\n",
    "\n",
    "print(\"Confusion matrix, out of\",total_samples,\"samples, with:\",total_spams,\"spams and:\",total_hams,\"hams\")\n",
    "\n",
    "print(\"true positive:\",true_pos, \",\",(true_pos/total_spams)*100,\"%\")\n",
    "print(\"true negative:\",true_neg, \",\",(true_neg/total_hams)*100,\"%\")\n",
    "\n",
    "print(\"false positive:\",false_pos, \",\",(false_pos/total_hams)*100,\"%\")\n",
    "print(\"false negative:\",false_neg, \",\",(false_neg/total_spams)*100,\"%\")\n",
    "\n",
    "\n",
    "print(classification_report(y_test,pred_label))\n",
    "print(confusion_matrix(y_test,pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing to scikit learn's naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98      1294\n",
      "         1.0       0.96      0.79      0.87       206\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1500\n",
      "   macro avg       0.97      0.89      0.92      1500\n",
      "weighted avg       0.97      0.97      0.97      1500\n",
      "\n",
      "[[1288    6]\n",
      " [  44  162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "NB = BernoulliNB()\n",
    "NB.fit(feature_vec,y_train)\n",
    "\n",
    "pred_vals = NB.predict(feature_vec_test )\n",
    "\n",
    "print(classification_report(y_test,pred_vals))\n",
    "print(confusion_matrix(y_test,pred_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a look at the performance\n",
    "\n",
    "We can see, that our algorithm performs a lot worse that the one provided in the scikit learn library; especially when it comes to false positives. False positives, also known as a type I error is a kind of error that we want to minimize as much as possible. this is because we dont want emails which are not spams being classified as spams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvement\n",
    "\n",
    "This model works relatively well but there are a lot of possible improvements that can be made to it. First, we can change the way that we make our dictionary. By using the library NLTK (Natural Language Toolkit) we can benefit from some of the implemented functions to exclude some words from our dictionary which appear frequently and have no real indication of wether a message truly is a spam or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
